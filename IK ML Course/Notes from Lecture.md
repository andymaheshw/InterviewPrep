 
#### Intro
Overall picture still remains the same but the underlying mechanic changes. 
Focus is more on the odeling of the ML task. 
* ETL: batch
* Discrepancy in online and offline data
* inference computation latency

#### online and on-device is the future of ML
* edge computing
* offline learning - periodically, batch size is millions, data seen repeatedly in epochs. 
* online learning - micro batch (hundreds of samples)
	* data only seen once 
#### Steps during ML System design
- Design Constraints - Scalability and Latency requirements
	- Number of users, transactions per second, predictions per second, data storage. 
	- make reasonable assumptions
	- can be collected at later step as well.
- Offline v. online metrics
	- offline - Useful for model eval, can be optimized directly, local view of the change, but not a global view. 
	- online metric - actual business metric, typically a/b testing, requires hypothesis testing, power analysis
- Good questions - building from scratch, or new? 
- deep dive into 1-2 critical components (15 min)
	- how to handle class imbalance. 

Design Search Results (like Google)
* Functional Reqs: 
	* latency/scalability tps
	* length of documents (and # of docs)
	* number of searches per second/per day
	* do we need to talk about batch processing vs. streaming - no
	* search query is given an input of the system
	* possible candidates for each quer is also an input
	* personalization of results. - no
	* how to define relevance
		* Length of session, # of clicks, human labeler
* Step 1 Scale/Latency Estimation
	* Number of Users: 100M
	* number of searches 10
	* QPS 10K
	* number of documents per query: 500-1000
	* typical end to end latency for search is 300 ms 
	* relevance componets to get 10-30 ms
		* These will influence our modeling choices
* Step 2: 
	* offline metrics: precision (true positives/all positives) how many articles should be listed per result over all results chosen), recall how many articles we posted over all articles, accuracy, nDCG
	* offline: https://towardsdatascience.com/evaluate-your-recommendation-engine-using-ndcg-759a851452d1
	* Mean Reciprocal Rank, Click Through Rate, Successful Sessions (good clicks, swell time), 
	* Reciprocal rank of first click/good clicks
* Step 3: High Level Architecture 
	* Block higlighted in yellow are main components
		* DOcument Index -> candidate retrieval -> filtering -> ranker -> ranker results -> training data -> model training !
	* Ranker Object -> ML model, object will be featurized -> prediction -> takes document/ query and ranks it. 
	* Training Data matches the ranker object
	* ![[Pasted image 20220130150602.png]]
	Step 4: Training Data:
		* Pagerank is not good because I don't know all details
		* query feature - unigram, igrams, character grams, 
		* intent based on historical data, 
		* length of query
		* frequency of the query to gague popularity
		* stop word removal
		* BMI25, TF-IDF
		* Jaccard SImilarity
		* Explore Exploit
		* Use 1M words, use locality preserving hashing. 
		* Negative sampling - take positive sample, randomly sample M documents that are unrelated. 
		* trend (last n weeks of data) + prediction week data from last week (seasonality)
		* validation data, current week's data
	* Step 5: Deep Dive - Ranker
		* Define label of each query document as a number between 0 - 1
		* use standard linear regression, predicted score to rank
		* loss function is MSE
		* Metrics: MSE, MAE, MAPE, P@K, R@K, NDCG
		* l1 - log of frequency of query
		* l2 - number of clicks
		* P@K - k = top 5,10, etc. precision is true positives/all trues
	* Step 5b - Model Publishing
		 - MSE P@K in a dashboard
		 - update version, checks if they passed
		 - publish infra metrics training time, validation examples, training
	 - Step 5c - Online testing
		 - A/B experiment with old and new model to test the hypothesis
		 - Look for evidence to reject null hypothesis
		 - bias in training data: 
			 - training data is obtrained from historical search impression logs
			 - model prediction is done on input of document scoring + filtering
			 - diff in data sets 
				 - distribution of features in training data is different than observed during prediction
				 - offline eval of any new model is biased due to data being generated by currently deployed model.
			 - Addressing bias
				 - Exploration policty - bucket for say 0.1 traffic
					 - randomly show a document - hurts user exp
					 - sample based on the score of the ranker
				 - log at different point in time/stage of the pipeline
					 - instead of logging features